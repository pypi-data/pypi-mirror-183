[
  "bleu.doctest::bleu.doctest",
  "bnc.doctest::bnc.doctest",
  "ccg.doctest::ccg.doctest",
  "ccg_semantics.doctest::ccg_semantics.doctest",
  "chat80.doctest::chat80.doctest",
  "childes.doctest::childes.doctest",
  "chunk.doctest::chunk.doctest",
  "classify.doctest::classify.doctest",
  "collections.doctest::collections.doctest",
  "collocations.doctest::collocations.doctest",
  "concordance.doctest::concordance.doctest",
  "corpus.doctest::corpus.doctest",
  "crubadan.doctest::crubadan.doctest",
  "data.doctest::data.doctest",
  "dependency.doctest::dependency.doctest",
  "discourse.doctest::discourse.doctest",
  "drt.doctest::drt.doctest",
  "featgram.doctest::featgram.doctest",
  "featstruct.doctest::featstruct.doctest",
  "framenet.doctest::framenet.doctest",
  "generate.doctest::generate.doctest",
  "gensim.doctest::gensim.doctest",
  "gluesemantics.doctest::gluesemantics.doctest",
  "gluesemantics_malt.doctest::gluesemantics_malt.doctest",
  "grammar.doctest::grammar.doctest",
  "grammartestsuites.doctest::grammartestsuites.doctest",
  "inference.doctest::inference.doctest",
  "internals.doctest::internals.doctest",
  "japanese.doctest::japanese.doctest",
  "lm.doctest::lm.doctest",
  "logic.doctest::logic.doctest",
  "meteor.doctest::meteor.doctest",
  "metrics.doctest::metrics.doctest",
  "misc.doctest::misc.doctest",
  "nonmonotonic.doctest::nonmonotonic.doctest",
  "paice.doctest::paice.doctest",
  "parse.doctest::parse.doctest",
  "portuguese_en.doctest::portuguese_en.doctest",
  "probability.doctest::probability.doctest",
  "propbank.doctest::propbank.doctest",
  "relextract.doctest::relextract.doctest",
  "resolution.doctest::resolution.doctest",
  "semantics.doctest::semantics.doctest",
  "sentiment.doctest::sentiment.doctest",
  "sentiwordnet.doctest::sentiwordnet.doctest",
  "simple.doctest::simple.doctest",
  "stem.doctest::stem.doctest",
  "tag.doctest::tag.doctest",
  "test_perf.doctest::test_perf.doctest",
  "tokenize.doctest::tokenize.doctest",
  "toolbox.doctest::toolbox.doctest",
  "translate.doctest::translate.doctest",
  "tree.doctest::tree.doctest",
  "treeprettyprinter.doctest::treeprettyprinter.doctest",
  "treetransforms.doctest::treetransforms.doctest",
  "unit/lm/test_counter.py::TestNgramCounter::test_N",
  "unit/lm/test_counter.py::TestNgramCounter::test_bigram_counts_seen_ngrams",
  "unit/lm/test_counter.py::TestNgramCounter::test_bigram_counts_unseen_ngrams",
  "unit/lm/test_counter.py::TestNgramCounter::test_counter_len_changes_with_lookup",
  "unit/lm/test_counter.py::TestNgramCounter::test_ngram_conditional_freqdist",
  "unit/lm/test_counter.py::TestNgramCounter::test_ngram_order_access_unigrams",
  "unit/lm/test_counter.py::TestNgramCounter::test_unigram_counts_completely_unseen_words",
  "unit/lm/test_counter.py::TestNgramCounter::test_unigram_counts_seen_words",
  "unit/lm/test_counter.py::TestNgramCounterTraining::test_empty_inputs[None]",
  "unit/lm/test_counter.py::TestNgramCounterTraining::test_empty_inputs[]",
  "unit/lm/test_counter.py::TestNgramCounterTraining::test_empty_inputs[case1]",
  "unit/lm/test_counter.py::TestNgramCounterTraining::test_train_on_bigrams",
  "unit/lm/test_counter.py::TestNgramCounterTraining::test_train_on_illegal_sentences",
  "unit/lm/test_counter.py::TestNgramCounterTraining::test_train_on_mix",
  "unit/lm/test_counter.py::TestNgramCounterTraining::test_train_on_unigrams",
  "unit/lm/test_models.py::test_absolute_discounting_trigram_score[c-None-0.05555555555555555]",
  "unit/lm/test_models.py::test_absolute_discounting_trigram_score[c-context3-0.16666666666666666]",
  "unit/lm/test_models.py::test_absolute_discounting_trigram_score[c-context4-0.375]",
  "unit/lm/test_models.py::test_absolute_discounting_trigram_score[c-context5-0.16666666666666666]",
  "unit/lm/test_models.py::test_absolute_discounting_trigram_score[y-None-0.16666666666666666]",
  "unit/lm/test_models.py::test_absolute_discounting_trigram_score[z-None-0.0]",
  "unit/lm/test_models.py::test_generate_None_text_seed",
  "unit/lm/test_models.py::test_generate_cycle",
  "unit/lm/test_models.py::test_generate_one_from_limiting_context",
  "unit/lm/test_models.py::test_generate_one_from_varied_context",
  "unit/lm/test_models.py::test_generate_one_no_context",
  "unit/lm/test_models.py::test_generate_oov_text_seed",
  "unit/lm/test_models.py::test_generate_with_text_seed",
  "unit/lm/test_models.py::test_kneserney_trigram_score[c-None-0.07142857142857142]",
  "unit/lm/test_models.py::test_kneserney_trigram_score[c-context3-0.17857142857142858]",
  "unit/lm/test_models.py::test_kneserney_trigram_score[c-context4-0.3839285714285714]",
  "unit/lm/test_models.py::test_kneserney_trigram_score[c-context5-0.17857142857142858]",
  "unit/lm/test_models.py::test_kneserney_trigram_score[y-None-0.21428571428571427]",
  "unit/lm/test_models.py::test_kneserney_trigram_score[z-None-0.0]",
  "unit/lm/test_models.py::test_laplace_bigram_entropy_perplexity",
  "unit/lm/test_models.py::test_laplace_bigram_score[a-None-0.13636363636363635]",
  "unit/lm/test_models.py::test_laplace_bigram_score[d-context0-0.2222222222222222]",
  "unit/lm/test_models.py::test_laplace_bigram_score[y-None-0.18181818181818182]",
  "unit/lm/test_models.py::test_laplace_bigram_score[z-None-0.045454545454545456]",
  "unit/lm/test_models.py::test_laplace_gamma",
  "unit/lm/test_models.py::test_lidstone_bigram_score[a-None-0.14189189189189189]",
  "unit/lm/test_models.py::test_lidstone_bigram_score[d-context0-0.6111111111111112]",
  "unit/lm/test_models.py::test_lidstone_bigram_score[y-None-0.20945945945945946]",
  "unit/lm/test_models.py::test_lidstone_bigram_score[z-None-0.006756756756756757]",
  "unit/lm/test_models.py::test_lidstone_entropy_perplexity",
  "unit/lm/test_models.py::test_lidstone_trigram_score[d-context0-0.6111111111111112]",
  "unit/lm/test_models.py::test_lidstone_trigram_score[d-context2-0.6111111111111112]",
  "unit/lm/test_models.py::test_lidstone_trigram_score[e-context1-0.05555555555555556]",
  "unit/lm/test_models.py::test_lidstone_trigram_score[e-context3-0.05555555555555556]",
  "unit/lm/test_models.py::test_mle_bigram_entropy_perplexity_seen",
  "unit/lm/test_models.py::test_mle_bigram_entropy_perplexity_unigrams",
  "unit/lm/test_models.py::test_mle_bigram_entropy_perplexity_unseen",
  "unit/lm/test_models.py::test_mle_bigram_logscore_for_zero_score",
  "unit/lm/test_models.py::test_mle_bigram_scores[a-None-0.14285714285714285]",
  "unit/lm/test_models.py::test_mle_bigram_scores[d-context0-1]",
  "unit/lm/test_models.py::test_mle_bigram_scores[d-context1-0]",
  "unit/lm/test_models.py::test_mle_bigram_scores[y-None-0.21428571428571427]",
  "unit/lm/test_models.py::test_mle_bigram_scores[z-None-0]",
  "unit/lm/test_models.py::test_mle_trigram_scores[a-None-0.1111111111111111]",
  "unit/lm/test_models.py::test_mle_trigram_scores[d-context0-1]",
  "unit/lm/test_models.py::test_mle_trigram_scores[d-context1-1]",
  "unit/lm/test_models.py::test_mle_trigram_scores[y-None-0.16666666666666666]",
  "unit/lm/test_models.py::test_mle_trigram_scores[z-None-0]",
  "unit/lm/test_models.py::test_stupid_backoff_trigram_score[c-None-0.05555555555555555]",
  "unit/lm/test_models.py::test_stupid_backoff_trigram_score[c-context3-0.5]",
  "unit/lm/test_models.py::test_stupid_backoff_trigram_score[c-context4-1.0]",
  "unit/lm/test_models.py::test_stupid_backoff_trigram_score[c-context5-0.2]",
  "unit/lm/test_models.py::test_stupid_backoff_trigram_score[y-None-0.16666666666666666]",
  "unit/lm/test_models.py::test_stupid_backoff_trigram_score[z-None-0.0]",
  "unit/lm/test_models.py::test_sums_to_1[<UNK>-absolute_discounting_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[<UNK>-kneserney_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[<UNK>-laplace_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[<UNK>-lidstone_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[<UNK>-mle_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[<UNK>-mle_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[<UNK>-stupid_backoff_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[<UNK>-wittenbell_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[<s>-absolute_discounting_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[<s>-kneserney_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[<s>-laplace_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[<s>-lidstone_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[<s>-mle_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[<s>-mle_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[<s>-stupid_backoff_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[<s>-wittenbell_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[a-absolute_discounting_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[a-kneserney_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[a-laplace_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[a-lidstone_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[a-mle_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[a-mle_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[a-stupid_backoff_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[a-wittenbell_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[b-absolute_discounting_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[b-kneserney_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[b-laplace_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[b-lidstone_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[b-mle_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[b-mle_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[b-stupid_backoff_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[b-wittenbell_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[c-absolute_discounting_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[c-kneserney_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[c-laplace_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[c-lidstone_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[c-mle_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[c-mle_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[c-stupid_backoff_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[c-wittenbell_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[d-absolute_discounting_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[d-kneserney_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[d-laplace_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[d-lidstone_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[d-mle_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[d-mle_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[d-stupid_backoff_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[d-wittenbell_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[e-absolute_discounting_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[e-kneserney_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[e-laplace_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[e-lidstone_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[e-mle_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[e-mle_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[e-stupid_backoff_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[e-wittenbell_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[r-absolute_discounting_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[r-kneserney_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[r-laplace_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[r-lidstone_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[r-mle_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[r-mle_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[r-stupid_backoff_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[r-wittenbell_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[w-absolute_discounting_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[w-kneserney_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[w-laplace_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[w-lidstone_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[w-mle_bigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[w-mle_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[w-stupid_backoff_trigram_model]",
  "unit/lm/test_models.py::test_sums_to_1[w-wittenbell_trigram_model]",
  "unit/lm/test_models.py::test_wittenbell_trigram_score[c-None-0.05555555555555555]",
  "unit/lm/test_models.py::test_wittenbell_trigram_score[c-context3-0.2777777777777778]",
  "unit/lm/test_models.py::test_wittenbell_trigram_score[c-context4-0.6388888888888888]",
  "unit/lm/test_models.py::test_wittenbell_trigram_score[c-context5-0.2777777777777778]",
  "unit/lm/test_models.py::test_wittenbell_trigram_score[y-None-0.16666666666666666]",
  "unit/lm/test_models.py::test_wittenbell_trigram_score[z-None-0.0]",
  "unit/lm/test_preprocessing.py::TestPreprocessing::test_padded_everygram_pipeline",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_counts_set_correctly",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_creation_with_counter",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_cutoff_setter_checks_value",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_cutoff_value_set_correctly",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_eqality",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_len_is_constant",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_lookup",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_lookup_None",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_lookup_empty_iterables",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_lookup_empty_str",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_lookup_int",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_lookup_iterables",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_lookup_recursive",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_membership_check_respects_cutoff",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_str",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_truthiness",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_unable_to_change_cutoff",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_update_empty_vocab",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_vocab_iter_respects_cutoff",
  "unit/lm/test_vocabulary.py::NgramModelVocabularyTests::test_vocab_len_respects_cutoff",
  "unit/test_aline.py::TestAline::test_align_I_ego",
  "unit/test_aline.py::TestAline::test_align_SiSipa_sesep",
  "unit/test_aline.py::TestAline::test_align_aend_ante",
  "unit/test_aline.py::TestAline::test_align_aet_ad",
  "unit/test_aline.py::TestAline::test_align_ahkohkwa_ahkeh",
  "unit/test_aline.py::TestAline::test_align_aj_awge",
  "unit/test_aline.py::TestAline::test_align_aj_ego",
  "unit/test_aline.py::TestAline::test_align_and_ante",
  "unit/test_aline.py::TestAline::test_align_arbol_arbre",
  "unit/test_aline.py::TestAline::test_align_asenja_asen",
  "unit/test_aline.py::TestAline::test_align_benir_venir",
  "unit/test_aline.py::TestAline::test_align_ber_vwar",
  "unit/test_aline.py::TestAline::test_align_bled_blyt",
  "unit/test_aline.py::TestAline::test_align_blow_flAre",
  "unit/test_aline.py::TestAline::test_align_blow_flare",
  "unit/test_aline.py::TestAline::test_align_boka_buS",
  "unit/test_aline.py::TestAline::test_align_daet_das",
  "unit/test_aline.py::TestAline::test_align_dethir_dir",
  "unit/test_aline.py::TestAline::test_align_dis_dIzes",
  "unit/test_aline.py::TestAline::test_align_dos_do",
  "unit/test_aline.py::TestAline::test_align_ear_auris",
  "unit/test_aline.py::TestAline::test_align_feder_fEder",
  "unit/test_aline.py::TestAline::test_align_fiS_piSkis",
  "unit/test_aline.py::TestAline::test_align_fleS_flajS",
  "unit/test_aline.py::TestAline::test_align_flow_fluere",
  "unit/test_aline.py::TestAline::test_align_ful_plenus",
  "unit/test_aline.py::TestAline::test_align_fut_fys",
  "unit/test_aline.py::TestAline::test_align_graes_gramen",
  "unit/test_aline.py::TestAline::test_align_haend_hant",
  "unit/test_aline.py::TestAline::test_align_haer_hAr",
  "unit/test_aline.py::TestAline::test_align_hart_herts",
  "unit/test_aline.py::TestAline::test_align_hart_kordis",
  "unit/test_aline.py::TestAline::test_align_horn_korny",
  "unit/test_aline.py::TestAline::test_align_ijt_edere",
  "unit/test_aline.py::TestAline::test_align_ir_Or",
  "unit/test_aline.py::TestAline::test_align_ir_awris",
  "unit/test_aline.py::TestAline::test_align_jo_Ze",
  "unit/test_aline.py::TestAline::test_align_kabetha_kap",
  "unit/test_aline.py::TestAline::test_align_ke_kwa",
  "unit/test_aline.py::TestAline::test_align_kinwawa_kenua",
  "unit/test_aline.py::TestAline::test_align_kjen_ki",
  "unit/test_aline.py::TestAline::test_align_korathon_koer",
  "unit/test_aline.py::TestAline::test_align_liver_lEber",
  "unit/test_aline.py::TestAline::test_align_loNG_laNG",
  "unit/test_aline.py::TestAline::test_align_maen_man",
  "unit/test_aline.py::TestAline::test_align_mawnten_mons",
  "unit/test_aline.py::TestAline::test_align_mawth_munt",
  "unit/test_aline.py::TestAline::test_align_meder_mAter",
  "unit/test_aline.py::TestAline::test_align_namesa_names",
  "unit/test_aline.py::TestAline::test_align_napewa_napew",
  "unit/test_aline.py::TestAline::test_align_nat_nixt",
  "unit/test_aline.py::TestAline::test_align_nejm_nomen",
  "unit/test_aline.py::TestAline::test_align_nij_genU",
  "unit/test_aline.py::TestAline::test_align_nij_knI",
  "unit/test_aline.py::TestAline::test_align_nina_nenah",
  "unit/test_aline.py::TestAline::test_align_njuw_nowus",
  "unit/test_aline.py::TestAline::test_align_nosotros_nu",
  "unit/test_aline.py::TestAline::test_align_nowz_nAze",
  "unit/test_aline.py::TestAline::test_align_okimawa_okemaw",
  "unit/test_aline.py::TestAline::test_align_ombre_om",
  "unit/test_aline.py::TestAline::test_align_pematesiweni_pematesewen",
  "unit/test_aline.py::TestAline::test_align_pje_pje",
  "unit/test_aline.py::TestAline::test_align_pluma_plym",
  "unit/test_aline.py::TestAline::test_align_pobre_povre",
  "unit/test_aline.py::TestAline::test_align_rawnd_rotundus",
  "unit/test_aline.py::TestAline::test_align_sit_sedere",
  "unit/test_aline.py::TestAline::test_align_sow_suere",
  "unit/test_aline.py::TestAline::test_align_star_stella",
  "unit/test_aline.py::TestAline::test_align_teNG_tsuNGe",
  "unit/test_aline.py::TestAline::test_align_thin_tenwis",
  "unit/test_aline.py::TestAline::test_align_thrij_tres",
  "unit/test_aline.py::TestAline::test_align_todos_tu",
  "unit/test_aline.py::TestAline::test_align_tooth_dentis",
  "unit/test_aline.py::TestAline::test_align_tres_trwa",
  "unit/test_aline.py::TestAline::test_align_tu_ty",
  "unit/test_aline.py::TestAline::test_align_tuwth_dentis",
  "unit/test_aline.py::TestAline::test_align_una_en",
  "unit/test_aline.py::TestAline::test_align_wapimini_wapemen",
  "unit/test_aline.py::TestAline::test_align_wat_vas",
  "unit/test_aline.py::TestAline::test_align_wen_unus",
  "unit/test_aline.py::TestAline::test_aline",
  "unit/test_aline.py::TestAline::test_aline_delta",
  "unit/test_aline.py::TestAline::test_aline_eat_edere",
  "unit/test_aline.py::TestAline::test_aline_fish_piscis",
  "unit/test_aline.py::TestAline::test_aline_flow_fluere",
  "unit/test_aline.py::TestAline::test_aline_star_stella",
  "unit/test_aline.py::test_aline",
  "unit/test_aline.py::test_aline_delta",
  "unit/test_bllip.py::TestBllipParser::test_parser_loads_a_valid_tree",
  "unit/test_bllip.py::TestBllipParser::test_tagged_parse_finds_matching_element",
  "unit/test_brill.py::TestBrill::test_brill_demo",
  "unit/test_brill.py::TestBrill::test_pos_template",
  "unit/test_cfd_mutation.py::TestEmptyCondFreq::test_increment",
  "unit/test_cfd_mutation.py::TestEmptyCondFreq::test_plot",
  "unit/test_cfd_mutation.py::TestEmptyCondFreq::test_tabulate",
  "unit/test_cfg2chomsky.py::ChomskyNormalFormForCFGTest::test_complex",
  "unit/test_cfg2chomsky.py::ChomskyNormalFormForCFGTest::test_simple",
  "unit/test_chunk.py::TestChunkRule::test_tag_pattern2re_pattern_quantifier",
  "unit/test_classify.py::test_megam",
  "unit/test_classify.py::test_tadm",
  "unit/test_collocations.py::test_bigram2",
  "unit/test_collocations.py::test_bigram3",
  "unit/test_collocations.py::test_bigram5",
  "unit/test_concordance.py::TestConcordance::test_concordance_lines",
  "unit/test_concordance.py::TestConcordance::test_concordance_list",
  "unit/test_concordance.py::TestConcordance::test_concordance_print",
  "unit/test_concordance.py::TestConcordance::test_concordance_width",
  "unit/test_corenlp.py::TestParserAPI::test_dependency_parser",
  "unit/test_corenlp.py::TestParserAPI::test_parse",
  "unit/test_corenlp.py::TestTaggerAPI::test_ner_tagger",
  "unit/test_corenlp.py::TestTaggerAPI::test_pos_tagger",
  "unit/test_corenlp.py::TestTaggerAPI::test_unexpected_tagtype",
  "unit/test_corenlp.py::TestTokenizerAPI::test_tokenize",
  "unit/test_corpora.py::TestCess::test_catalan",
  "unit/test_corpora.py::TestCess::test_esp",
  "unit/test_corpora.py::TestCoNLL2007::test_parsed_sents",
  "unit/test_corpora.py::TestCoNLL2007::test_sents",
  "unit/test_corpora.py::TestFloresta::test_words",
  "unit/test_corpora.py::TestIndian::test_tagged_words",
  "unit/test_corpora.py::TestIndian::test_words",
  "unit/test_corpora.py::TestMWAPPDB::test_entries",
  "unit/test_corpora.py::TestMWAPPDB::test_fileids",
  "unit/test_corpora.py::TestPTB::test_categories",
  "unit/test_corpora.py::TestPTB::test_category_words",
  "unit/test_corpora.py::TestPTB::test_fileids",
  "unit/test_corpora.py::TestPTB::test_news_fileids",
  "unit/test_corpora.py::TestPTB::test_tagged_words",
  "unit/test_corpora.py::TestPTB::test_words",
  "unit/test_corpora.py::TestSinicaTreebank::test_parsed_sents",
  "unit/test_corpora.py::TestSinicaTreebank::test_sents",
  "unit/test_corpora.py::TestUdhr::test_polish_encoding",
  "unit/test_corpora.py::TestUdhr::test_raw_unicode",
  "unit/test_corpora.py::TestUdhr::test_words",
  "unit/test_corpus_views.py::TestCorpusViews::test_correct_length",
  "unit/test_corpus_views.py::TestCorpusViews::test_correct_values",
  "unit/test_data.py::test_find_raises_exception",
  "unit/test_data.py::test_find_raises_exception_with_full_resource_name",
  "unit/test_disagreement.py::TestDisagreement::test_advanced",
  "unit/test_disagreement.py::TestDisagreement::test_advanced2",
  "unit/test_disagreement.py::TestDisagreement::test_easy",
  "unit/test_disagreement.py::TestDisagreement::test_easy2",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[-text-1-expecteds21]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[-text-1-expecteds24]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[-text-2-expecteds22]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[-text-2-expecteds25]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[abc-ca-1-expecteds0]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[abc-ca-5-expecteds1]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[acbdef-abcdef-1-expecteds6]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[acbdef-abcdef-2-expecteds7]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[duplicated-duuplibated-1-expecteds21]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[duplicated-duuplibated-2-expecteds22]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[duplicated-duuplicated-1-expecteds18]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[duplicated-duuplicated-2-expecteds19]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[kitten-sitting-1-expecteds16]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[kitten-sitting-2-expecteds17]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[language-lnaugage-1-expecteds10]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[language-lnaugage-2-expecteds11]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[lnaguaeg-language-1-expecteds8]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[lnaguaeg-language-2-expecteds9]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[lnaugage-language-1-expecteds10]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[lnaugage-language-2-expecteds11]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[lngauage-language-1-expecteds12]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[lngauage-language-2-expecteds13]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[rain-shine-1-expecteds4]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[rain-shine-2-expecteds5]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[rain-shine-5-expecteds5]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[text-text-1-expecteds26]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[text-text-2-expecteds27]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[very duplicated-very duuplibateed-2-expecteds23]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[very duplicated-very duuplicateed-2-expecteds20]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[wants-Wasp-1-expecteds2]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[wants-Wasp-2-expecteds3]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[wants-swim-1-expecteds14]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[wants-swim-2-expecteds15]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[wants-wasp-1-expecteds2]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[wants-wasp-2-expecteds3]",
  "unit/test_distance.py::TestEditDistance::test_with_transpositions[wants-wasp-5-expecteds3]",
  "unit/test_distance.py::TestEditDistanceABC::test_with_transpositions[abc-ca-1-False-3]",
  "unit/test_distance.py::TestEditDistanceABC::test_with_transpositions[abc-ca-1-True-2]",
  "unit/test_distance.py::TestEditDistanceABC::test_with_transpositions[abc-ca-2-True-3]",
  "unit/test_distance.py::TestEditDistanceABC::test_with_transpositions[abc-ca-3-True-3]",
  "unit/test_distance.py::TestEditDistanceABC::test_with_transpositions[abc-ca-5-False-3]",
  "unit/test_distance.py::TestEditDistanceABC::test_with_transpositions[abc-ca-5-True-2]",
  "unit/test_distance.py::TestEditDistanceABC::test_with_transpositions[rain-shine-1-False-3]",
  "unit/test_distance.py::TestEditDistanceABC::test_with_transpositions[rain-shine-1-True-3]",
  "unit/test_distance.py::TestEditDistanceABC::test_with_transpositions[rain-shine-2-False-5]",
  "unit/test_distance.py::TestEditDistanceABC::test_with_transpositions[rain-shine-2-True-5]",
  "unit/test_distance.py::TestEditDistanceABC::test_with_transpositions[wants-Wasp-1-False-3]",
  "unit/test_distance.py::TestEditDistanceABC::test_with_transpositions[wants-Wasp-1-False-4]",
  "unit/test_distance.py::TestEditDistanceABC::test_with_transpositions[wants-Wasp-1-True-3]",
  "unit/test_distance.py::TestEditDistanceABC::test_with_transpositions[wants-Wasp-1-True-4]",
  "unit/test_distance.py::TestEditDistanceABC::test_with_transpositions[wants-Wasp-2-False-5]",
  "unit/test_distance.py::TestEditDistanceABC::test_with_transpositions[wants-Wasp-2-True-4]",
  "unit/test_downloader.py::test_downloader_using_existing_parent_download_dir",
  "unit/test_downloader.py::test_downloader_using_non_existing_parent_download_dir",
  "unit/test_freqdist.py::test_iterating_returns_an_iterator_ordered_by_frequency",
  "unit/test_hmm.py::test_backward_probability",
  "unit/test_hmm.py::test_forward_probability",
  "unit/test_hmm.py::test_forward_probability2",
  "unit/test_json2csv_corpus.py::test_file_is_wrong",
  "unit/test_json2csv_corpus.py::test_retweet_original_tweet",
  "unit/test_json2csv_corpus.py::test_textoutput",
  "unit/test_json2csv_corpus.py::test_tweet_hashtag",
  "unit/test_json2csv_corpus.py::test_tweet_media",
  "unit/test_json2csv_corpus.py::test_tweet_metadata",
  "unit/test_json2csv_corpus.py::test_tweet_place",
  "unit/test_json2csv_corpus.py::test_tweet_place_boundingbox",
  "unit/test_json2csv_corpus.py::test_tweet_url",
  "unit/test_json2csv_corpus.py::test_tweet_usermention",
  "unit/test_json2csv_corpus.py::test_user_metadata",
  "unit/test_json2csv_corpus.py::test_userurl",
  "unit/test_json_serialization.py::TestJSONSerialization::test_affix_tagger",
  "unit/test_json_serialization.py::TestJSONSerialization::test_brill_tagger",
  "unit/test_json_serialization.py::TestJSONSerialization::test_default_tagger",
  "unit/test_json_serialization.py::TestJSONSerialization::test_ngram_taggers",
  "unit/test_json_serialization.py::TestJSONSerialization::test_perceptron_tagger",
  "unit/test_json_serialization.py::TestJSONSerialization::test_regexp_tagger",
  "unit/test_metrics.py::TestLikelihoodRatio::test_lr_bigram",
  "unit/test_metrics.py::TestLikelihoodRatio::test_lr_quadgram",
  "unit/test_metrics.py::TestLikelihoodRatio::test_lr_trigram",
  "unit/test_naivebayes.py::NaiveBayesClassifierTest::test_simple",
  "unit/test_nombank.py::NombankDemo::test_framefiles_fileids",
  "unit/test_nombank.py::NombankDemo::test_instance",
  "unit/test_nombank.py::NombankDemo::test_numbers",
  "unit/test_phoneRegex.py::tweetTokenizer::test_falsePhoneRegex",
  "unit/test_phoneRegex.py::tweetTokenizer::test_truePhoneRegex",
  "unit/test_pl196x.py::TestCorpusViews::test_corpus_reader",
  "unit/test_pos_tag.py::TestPosTag::test_pos_tag_eng",
  "unit/test_pos_tag.py::TestPosTag::test_pos_tag_eng_universal",
  "unit/test_pos_tag.py::TestPosTag::test_pos_tag_rus",
  "unit/test_pos_tag.py::TestPosTag::test_pos_tag_rus_universal",
  "unit/test_pos_tag.py::TestPosTag::test_pos_tag_unknown_lang",
  "unit/test_pos_tag.py::TestPosTag::test_unspecified_lang",
  "unit/test_ribes.py::test_empty_worder",
  "unit/test_ribes.py::test_no_zero_div",
  "unit/test_ribes.py::test_one_worder",
  "unit/test_ribes.py::test_ribes",
  "unit/test_ribes.py::test_ribes_empty_worder",
  "unit/test_ribes.py::test_ribes_one_worder",
  "unit/test_ribes.py::test_ribes_two_worder",
  "unit/test_ribes.py::test_two_worder",
  "unit/test_rte_classify.py::TestRTEClassifier::test_feature_extractor_object",
  "unit/test_rte_classify.py::TestRTEClassifier::test_rte_classification_with_megam",
  "unit/test_rte_classify.py::TestRTEClassifier::test_rte_classification_without_megam",
  "unit/test_rte_classify.py::TestRTEClassifier::test_rte_feature_extraction",
  "unit/test_seekable_unicode_stream_reader.py::test_reader[    This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n        This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n        This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n        This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n        This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n        This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n        This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n        This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n        This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n        This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n        This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n        This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n        This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n        This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n        This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n        This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n        This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n        This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n        This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n        This is a larger file.  It has some lines that are longer     than 72 characters.  It's got lots of repetition.  Here's     some unicode chars: \\xee \\u0123 \\uffe3 \\ueeee \\u2345\\n\\n    How fun!  Let's repeat it twenty times.\\n    ]",
  "unit/test_seekable_unicode_stream_reader.py::test_reader[    This is a test file.\\n    Here's a blank line:\\n\\n    And here's some unicode: \\xee \\u0123 \\uffe3\\n    ]",
  "unit/test_seekable_unicode_stream_reader.py::test_reader[    This is a test file.\\n    Unicode characters: \\xf3 \\u2222 \\u3333\\u4444 \\u5555\\n    ]",
  "unit/test_seekable_unicode_stream_reader.py::test_reader[This file can be encoded with latin1. \\x83]",
  "unit/test_seekable_unicode_stream_reader.py::test_reader[\\n    This is a test file.\\n    It is fairly short.\\n    ]",
  "unit/test_seekable_unicode_stream_reader.py::test_reader_stream_closes_when_deleted",
  "unit/test_senna.py::TestSennaPipeline::test_senna_pipeline",
  "unit/test_senna.py::TestSennaTagger::test_senna_chunk_tagger",
  "unit/test_senna.py::TestSennaTagger::test_senna_ner_tagger",
  "unit/test_senna.py::TestSennaTagger::test_senna_tagger",
  "unit/test_stem.py::PorterTest::test_lowercase_option",
  "unit/test_stem.py::PorterTest::test_oed_bug",
  "unit/test_stem.py::PorterTest::test_vocabulary_martin_mode",
  "unit/test_stem.py::PorterTest::test_vocabulary_nltk_mode",
  "unit/test_stem.py::PorterTest::test_vocabulary_original_mode",
  "unit/test_stem.py::SnowballTest::test_arabic",
  "unit/test_stem.py::SnowballTest::test_german",
  "unit/test_stem.py::SnowballTest::test_russian",
  "unit/test_stem.py::SnowballTest::test_short_strings_bug",
  "unit/test_stem.py::SnowballTest::test_spanish",
  "unit/test_tag.py::test_basic",
  "unit/test_tgrep.py::TestSequenceFunctions::test_bad_operator",
  "unit/test_tgrep.py::TestSequenceFunctions::test_comments",
  "unit/test_tgrep.py::TestSequenceFunctions::test_examples",
  "unit/test_tgrep.py::TestSequenceFunctions::test_labeled_nodes",
  "unit/test_tgrep.py::TestSequenceFunctions::test_multiple_conjs",
  "unit/test_tgrep.py::TestSequenceFunctions::test_node_encoding",
  "unit/test_tgrep.py::TestSequenceFunctions::test_node_nocase",
  "unit/test_tgrep.py::TestSequenceFunctions::test_node_noleaves",
  "unit/test_tgrep.py::TestSequenceFunctions::test_node_printing",
  "unit/test_tgrep.py::TestSequenceFunctions::test_node_quoted",
  "unit/test_tgrep.py::TestSequenceFunctions::test_node_regex",
  "unit/test_tgrep.py::TestSequenceFunctions::test_node_regex_2",
  "unit/test_tgrep.py::TestSequenceFunctions::test_node_simple",
  "unit/test_tgrep.py::TestSequenceFunctions::test_node_tree_position",
  "unit/test_tgrep.py::TestSequenceFunctions::test_rel_precedence",
  "unit/test_tgrep.py::TestSequenceFunctions::test_rel_sister_nodes",
  "unit/test_tgrep.py::TestSequenceFunctions::test_tokenize_encoding",
  "unit/test_tgrep.py::TestSequenceFunctions::test_tokenize_examples",
  "unit/test_tgrep.py::TestSequenceFunctions::test_tokenize_link_types",
  "unit/test_tgrep.py::TestSequenceFunctions::test_tokenize_macros",
  "unit/test_tgrep.py::TestSequenceFunctions::test_tokenize_node_labels",
  "unit/test_tgrep.py::TestSequenceFunctions::test_tokenize_nodenames",
  "unit/test_tgrep.py::TestSequenceFunctions::test_tokenize_quoting",
  "unit/test_tgrep.py::TestSequenceFunctions::test_tokenize_segmented_patterns",
  "unit/test_tgrep.py::TestSequenceFunctions::test_tokenize_simple",
  "unit/test_tgrep.py::TestSequenceFunctions::test_trailing_semicolon",
  "unit/test_tgrep.py::TestSequenceFunctions::test_use_macros",
  "unit/test_tgrep.py::TestSequenceFunctions::tests_rel_dominance",
  "unit/test_tgrep.py::TestSequenceFunctions::tests_rel_indexed_children",
  "unit/test_tokenize.py::TestTokenize::test_emoji_tokenizer",
  "unit/test_tokenize.py::TestTokenize::test_legality_principle_syllable_tokenizer",
  "unit/test_tokenize.py::TestTokenize::test_pad_asterisk",
  "unit/test_tokenize.py::TestTokenize::test_pad_dotdot",
  "unit/test_tokenize.py::TestTokenize::test_phone_tokenizer",
  "unit/test_tokenize.py::TestTokenize::test_punkt_debug_decisions_custom_end",
  "unit/test_tokenize.py::TestTokenize::test_punkt_debug_decisions_dot",
  "unit/test_tokenize.py::TestTokenize::test_punkt_debug_decisions_exclamation_mark",
  "unit/test_tokenize.py::TestTokenize::test_punkt_debug_decisions_no_split",
  "unit/test_tokenize.py::TestTokenize::test_punkt_pair_iter",
  "unit/test_tokenize.py::TestTokenize::test_punkt_pair_iter_handles_stop_iteration_exception",
  "unit/test_tokenize.py::TestTokenize::test_punkt_tokenize_custom_lang_vars",
  "unit/test_tokenize.py::TestTokenize::test_punkt_tokenize_no_custom_lang_vars",
  "unit/test_tokenize.py::TestTokenize::test_punkt_tokenize_words_handles_stop_iteration_exception",
  "unit/test_tokenize.py::TestTokenize::test_remove_handle",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[1. This is R .\\n2. This is A .\\n3. That's all-expected3]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[1. This is R .\\n2. This is A .\\n3. That's all-expected6]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[1. This is R .\\n2. This is A .\\n3. That's all-expected7]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[1. This is R .\\n2. This is A .\\n3. That's all-expected9]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[1. This is R .\\t2. This is A .\\t3. That's all-expected10]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[Hello.\\tThere-expected11]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[This! That-expected8]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[This!!! That-expected6]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[This!!! That-expected7]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[This. ,. That-expected5]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[This. ,. That-expected6]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[This. . . That-expected1]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[This. . . That-expected2]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[This. .. That-expected4]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[This. .. That-expected5]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[This.. . That-expected3]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[This.. . That-expected4]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[This... That-expected1]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[This... That-expected2]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[This... That-expected3]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[This..... That-expected2]",
  "unit/test_tokenize.py::TestTokenize::test_sent_tokenize[this is a test. . new sentence.-expected0]",
  "unit/test_tokenize.py::TestTokenize::test_sonority_sequencing_syllable_tokenizer",
  "unit/test_tokenize.py::TestTokenize::test_stanford_segmenter_arabic",
  "unit/test_tokenize.py::TestTokenize::test_stanford_segmenter_chinese",
  "unit/test_tokenize.py::TestTokenize::test_syllable_tokenizer_numbers",
  "unit/test_tokenize.py::TestTokenize::test_treebank_span_tokenizer",
  "unit/test_tokenize.py::TestTokenize::test_tweet_tokenizer",
  "unit/test_tokenize.py::TestTokenize::test_tweet_tokenizer_expanded[@remy: This is waaaaayyyy too much for you!!!!!! 01064042430-expecteds2]",
  "unit/test_tokenize.py::TestTokenize::test_tweet_tokenizer_expanded[False-@remy: This is waaaaayyyy too much for you!!!!!! 01064042430-expected5]",
  "unit/test_tokenize.py::TestTokenize::test_tweet_tokenizer_expanded[False-My text 0106404243030 is great text-expected1]",
  "unit/test_tokenize.py::TestTokenize::test_tweet_tokenizer_expanded[False-My text 0106404243030-expected1]",
  "unit/test_tokenize.py::TestTokenize::test_tweet_tokenizer_expanded[False-My ticket id is 1234543124123-expected3]",
  "unit/test_tokenize.py::TestTokenize::test_tweet_tokenizer_expanded[My favourite substraction is 240 - 1353.-expecteds7]",
  "unit/test_tokenize.py::TestTokenize::test_tweet_tokenizer_expanded[My number is (393)  928 -3010, except it's not.-expecteds5]",
  "unit/test_tokenize.py::TestTokenize::test_tweet_tokenizer_expanded[My number is 06-46124080, except it's not.-expecteds3]",
  "unit/test_tokenize.py::TestTokenize::test_tweet_tokenizer_expanded[My number is 601-984-4813, except it's not.-expecteds4]",
  "unit/test_tokenize.py::TestTokenize::test_tweet_tokenizer_expanded[My text 0106404243030 is great text-expecteds0]",
  "unit/test_tokenize.py::TestTokenize::test_tweet_tokenizer_expanded[My ticket id is 1234543124123-expecteds1]",
  "unit/test_tokenize.py::TestTokenize::test_tweet_tokenizer_expanded[The product identification number is 48103284512.-expecteds6]",
  "unit/test_tokenize.py::TestTokenize::test_tweet_tokenizer_expanded[True-@remy: This is waaaaayyyy too much for you!!!!!! 01064042430-expected4]",
  "unit/test_tokenize.py::TestTokenize::test_tweet_tokenizer_expanded[True-My text 0106404243030 is great text-expected0]",
  "unit/test_tokenize.py::TestTokenize::test_tweet_tokenizer_expanded[True-My text 0106404243030-expected0]",
  "unit/test_tokenize.py::TestTokenize::test_tweet_tokenizer_expanded[True-My ticket id is 1234543124123-expected2]",
  "unit/test_tokenize.py::TestTokenize::test_word_tokenize",
  "unit/test_twitter_auth.py::TestCredentials::test_correct_file",
  "unit/test_twitter_auth.py::TestCredentials::test_environment",
  "unit/test_twitter_auth.py::TestCredentials::test_scenarios_that_should_raise_errors[kwargs0]",
  "unit/test_twitter_auth.py::TestCredentials::test_scenarios_that_should_raise_errors[kwargs1]",
  "unit/test_twitter_auth.py::TestCredentials::test_scenarios_that_should_raise_errors[kwargs2]",
  "unit/test_twitter_auth.py::TestCredentials::test_scenarios_that_should_raise_errors[kwargs3]",
  "unit/test_twitter_auth.py::TestCredentials::test_scenarios_that_should_raise_errors[kwargs4]",
  "unit/test_twitter_auth.py::TestCredentials::test_scenarios_that_should_raise_errors[kwargs5]",
  "unit/test_twitter_auth.py::TestCredentials::test_scenarios_that_should_raise_errors[kwargs6]",
  "unit/test_twitter_auth.py::TestCredentials::test_scenarios_that_should_raise_errors[kwargs7]",
  "unit/test_util.py::TestEverygrams::test_everygrams_max_len",
  "unit/test_util.py::TestEverygrams::test_everygrams_min_len",
  "unit/test_util.py::TestEverygrams::test_everygrams_pad_left",
  "unit/test_util.py::TestEverygrams::test_everygrams_pad_right",
  "unit/test_util.py::TestEverygrams::test_everygrams_without_padding",
  "unit/test_util.py::test_everygrams_max_len",
  "unit/test_util.py::test_everygrams_min_len",
  "unit/test_util.py::test_everygrams_pad_left",
  "unit/test_util.py::test_everygrams_pad_right",
  "unit/test_util.py::test_everygrams_without_padding",
  "unit/test_util.py::test_usage_on_builtin",
  "unit/test_util.py::test_usage_with_cls",
  "unit/test_util.py::test_usage_with_self",
  "unit/test_wordnet.py::WordnNetDemo::test_antonyms",
  "unit/test_wordnet.py::WordnNetDemo::test_derivationally_related_forms",
  "unit/test_wordnet.py::WordnNetDemo::test_domains",
  "unit/test_wordnet.py::WordnNetDemo::test_hyperhyponyms",
  "unit/test_wordnet.py::WordnNetDemo::test_in_topic_domains",
  "unit/test_wordnet.py::WordnNetDemo::test_iterable_type_for_all_lemma_names",
  "unit/test_wordnet.py::WordnNetDemo::test_lch",
  "unit/test_wordnet.py::WordnNetDemo::test_meronyms_holonyms",
  "unit/test_wordnet.py::WordnNetDemo::test_misc_relations",
  "unit/test_wordnet.py::WordnNetDemo::test_omw_lemma_no_trailing_underscore",
  "unit/test_wordnet.py::WordnNetDemo::test_retrieve_synset",
  "unit/test_wordnet.py::WordnNetDemo::test_retrieve_synsets",
  "unit/test_wordnet.py::WordnNetDemo::test_wordnet_similarities",
  "unit/translate/test_bleu.py::TestBLEU::test_brevity_penalty",
  "unit/translate/test_bleu.py::TestBLEU::test_full_matches",
  "unit/translate/test_bleu.py::TestBLEU::test_modified_precision",
  "unit/translate/test_bleu.py::TestBLEU::test_partial_matches_hypothesis_longer_than_reference",
  "unit/translate/test_bleu.py::TestBLEU::test_zero_matches",
  "unit/translate/test_bleu.py::TestBLEUFringeCases::test_case_where_n_is_bigger_than_hypothesis_length",
  "unit/translate/test_bleu.py::TestBLEUFringeCases::test_empty_hypothesis",
  "unit/translate/test_bleu.py::TestBLEUFringeCases::test_empty_references",
  "unit/translate/test_bleu.py::TestBLEUFringeCases::test_empty_references_and_hypothesis",
  "unit/translate/test_bleu.py::TestBLEUFringeCases::test_length_one_hypothesis",
  "unit/translate/test_bleu.py::TestBLEUFringeCases::test_reference_or_hypothesis_shorter_than_fourgrams",
  "unit/translate/test_bleu.py::TestBLEUWithBadSentence::test_corpus_bleu_with_bad_sentence",
  "unit/translate/test_bleu.py::TestBLEUWithMultipleWeights::test_corpus_bleu_with_multiple_weights",
  "unit/translate/test_bleu.py::TestBLEUvsMteval13a::test_corpus_bleu",
  "unit/translate/test_gdfa.py::TestGDFA::test_from_eflomal_outputs",
  "unit/translate/test_ibm1.py::TestIBMModel1::test_prob_t_a_given_s",
  "unit/translate/test_ibm1.py::TestIBMModel1::test_set_uniform_translation_probabilities",
  "unit/translate/test_ibm1.py::TestIBMModel1::test_set_uniform_translation_probabilities_of_non_domain_values",
  "unit/translate/test_ibm2.py::TestIBMModel2::test_prob_t_a_given_s",
  "unit/translate/test_ibm2.py::TestIBMModel2::test_set_uniform_alignment_probabilities",
  "unit/translate/test_ibm2.py::TestIBMModel2::test_set_uniform_alignment_probabilities_of_non_domain_values",
  "unit/translate/test_ibm3.py::TestIBMModel3::test_prob_t_a_given_s",
  "unit/translate/test_ibm3.py::TestIBMModel3::test_set_uniform_distortion_probabilities",
  "unit/translate/test_ibm3.py::TestIBMModel3::test_set_uniform_distortion_probabilities_of_non_domain_values",
  "unit/translate/test_ibm4.py::TestIBMModel4::test_prob_t_a_given_s",
  "unit/translate/test_ibm4.py::TestIBMModel4::test_set_uniform_distortion_probabilities_of_max_displacements",
  "unit/translate/test_ibm4.py::TestIBMModel4::test_set_uniform_distortion_probabilities_of_non_domain_values",
  "unit/translate/test_ibm5.py::TestIBMModel5::test_prob_t_a_given_s",
  "unit/translate/test_ibm5.py::TestIBMModel5::test_prune",
  "unit/translate/test_ibm5.py::TestIBMModel5::test_set_uniform_vacancy_probabilities_of_max_displacements",
  "unit/translate/test_ibm5.py::TestIBMModel5::test_set_uniform_vacancy_probabilities_of_non_domain_values",
  "unit/translate/test_ibm_model.py::TestIBMModel::test_best_model2_alignment",
  "unit/translate/test_ibm_model.py::TestIBMModel::test_best_model2_alignment_does_not_change_pegged_alignment",
  "unit/translate/test_ibm_model.py::TestIBMModel::test_best_model2_alignment_handles_empty_src_sentence",
  "unit/translate/test_ibm_model.py::TestIBMModel::test_best_model2_alignment_handles_empty_trg_sentence",
  "unit/translate/test_ibm_model.py::TestIBMModel::test_best_model2_alignment_handles_fertile_words",
  "unit/translate/test_ibm_model.py::TestIBMModel::test_hillclimb",
  "unit/translate/test_ibm_model.py::TestIBMModel::test_neighboring_finds_neighbor_alignments",
  "unit/translate/test_ibm_model.py::TestIBMModel::test_neighboring_returns_neighbors_with_pegged_alignment",
  "unit/translate/test_ibm_model.py::TestIBMModel::test_neighboring_sets_neighbor_alignment_info",
  "unit/translate/test_ibm_model.py::TestIBMModel::test_sample",
  "unit/translate/test_ibm_model.py::TestIBMModel::test_vocabularies_are_initialized",
  "unit/translate/test_ibm_model.py::TestIBMModel::test_vocabularies_are_initialized_even_with_empty_corpora",
  "unit/translate/test_meteor.py::TestMETEOR::test_candidate_type_check",
  "unit/translate/test_meteor.py::TestMETEOR::test_meteor",
  "unit/translate/test_meteor.py::TestMETEOR::test_preprocess",
  "unit/translate/test_meteor.py::TestMETEOR::test_reference_type_check",
  "unit/translate/test_nist.py::TestNIST::test_sentence_nist",
  "unit/translate/test_stack_decoder.py::TestHypothesis::test_total_translated_words",
  "unit/translate/test_stack_decoder.py::TestHypothesis::test_translated_positions",
  "unit/translate/test_stack_decoder.py::TestHypothesis::test_translation_so_far",
  "unit/translate/test_stack_decoder.py::TestHypothesis::test_translation_so_far_for_empty_hypothesis",
  "unit/translate/test_stack_decoder.py::TestHypothesis::test_untranslated_spans",
  "unit/translate/test_stack_decoder.py::TestHypothesis::test_untranslated_spans_for_empty_hypothesis",
  "unit/translate/test_stack_decoder.py::TestStack::test_best_returns_none_when_stack_is_empty",
  "unit/translate/test_stack_decoder.py::TestStack::test_best_returns_the_best_hypothesis",
  "unit/translate/test_stack_decoder.py::TestStack::test_push_bumps_off_worst_hypothesis_when_stack_is_full",
  "unit/translate/test_stack_decoder.py::TestStack::test_push_does_not_add_hypothesis_that_falls_below_beam_threshold",
  "unit/translate/test_stack_decoder.py::TestStack::test_push_removes_hypotheses_that_fall_below_beam_threshold",
  "unit/translate/test_stack_decoder.py::TestStackDecoder::test_compute_future_costs",
  "unit/translate/test_stack_decoder.py::TestStackDecoder::test_compute_future_costs_for_phrases_not_in_phrase_table",
  "unit/translate/test_stack_decoder.py::TestStackDecoder::test_distortion_score",
  "unit/translate/test_stack_decoder.py::TestStackDecoder::test_distortion_score_of_first_expansion",
  "unit/translate/test_stack_decoder.py::TestStackDecoder::test_find_all_src_phrases",
  "unit/translate/test_stack_decoder.py::TestStackDecoder::test_future_score",
  "unit/translate/test_stack_decoder.py::TestStackDecoder::test_valid_phrases",
  "util.doctest::util.doctest",
  "wordnet.doctest::wordnet.doctest",
  "wordnet_lch.doctest::wordnet_lch.doctest",
  "wsd.doctest::wsd.doctest"
]