<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>redvox.common.api_reader API documentation</title>
<meta name="description" content="Read Redvox data from a single directory
Data files can be either API 900 or API 1000 data formats" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>redvox.common.api_reader</code></h1>
</header>
<section id="section-intro">
<p>Read Redvox data from a single directory
Data files can be either API 900 or API 1000 data formats</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Read Redvox data from a single directory
Data files can be either API 900 or API 1000 data formats
&#34;&#34;&#34;
from typing import List, Optional
from datetime import timedelta, datetime
import multiprocessing
import multiprocessing.pool

import pyarrow as pa
import psutil

import redvox.settings as settings
import redvox.api1000.proto.redvox_api_m_pb2 as api_m
from redvox.common import offset_model, io,\
    api_conversions as ac,\
    file_statistics as fs
from redvox.common.parallel_utils import maybe_parallel_map
from redvox.common.station import Station
from redvox.common.errors import RedVoxExceptions


id_py_stct = pa.struct([(&#34;id&#34;, pa.string()), (&#34;uuid&#34;, pa.string()), (&#34;start_time&#34;, pa.float64()),
                        ])
meta_py_stct = pa.struct([(&#34;api&#34;, pa.float64()), (&#34;sub_api&#34;, pa.float64()), (&#34;make&#34;, pa.string()),
                          (&#34;model&#34;, pa.string()), (&#34;os&#34;, pa.int64()), (&#34;os_version&#34;, pa.string()),
                          (&#34;app&#34;, pa.string()), (&#34;app_version&#34;, pa.string()), (&#34;is_private&#34;, pa.bool_()),
                          (&#34;packet_duration_s&#34;, pa.float64()), (&#34;station_description&#34;, pa.string()),
                          ])


PERCENT_FREE_MEM_USE = .8  # Percentage of total free memory to use when creating stations (1. is 100%)


class ApiReader:
    &#34;&#34;&#34;
    Reads data from api 900 or api 1000 format, converting all data read into RedvoxPacketM for
    ease of comparison and use.

    Properties:
        filter: io.ReadFilter with the station ids, start and end time, start and end time padding, and
        types of files to read

        base_dir: str of the directory containing all the files to read

        structured_dir: bool, if True, the base_dir contains a specific directory structure used by the
        respective api formats.  If False, base_dir only has the data files.  Default False.

        files_index: io.Index of the files that match the filter that are in base_dir

        index_summary: io.IndexSummary of the filtered data

        debug: bool, if True, output additional information during function execution.  Default False.
    &#34;&#34;&#34;

    def __init__(
        self,
        base_dir: str,
        structured_dir: bool = False,
        read_filter: io.ReadFilter = None,
        debug: bool = False,
        pool: Optional[multiprocessing.pool.Pool] = None,
    ):
        &#34;&#34;&#34;
        Initialize the ApiReader object

        :param base_dir: directory containing the files to read
        :param structured_dir: if True, base_dir contains a specific directory structure used by the respective
                                api formats.  If False, base_dir only has the data files.  Default False.
        :param read_filter: ReadFilter for the data files, if None, get everything.  Default None
        :param debug: if True, output program warnings/errors during function execution.  Default False.
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = (
            multiprocessing.Pool() if pool is None else pool
        )

        if read_filter:
            self.filter = read_filter
            if self.filter.station_ids:
                self.filter.station_ids = set(self.filter.station_ids)
        else:
            self.filter = io.ReadFilter()
        self.base_dir = base_dir
        self.structured_dir = structured_dir
        self.debug = debug
        self.errors = RedVoxExceptions(&#34;APIReader&#34;)
        self.files_index = self._get_all_files(_pool)
        self.index_summary = io.IndexSummary.from_index(self._flatten_files_index())
        if len(self.files_index) &gt; 0:
            mem_split_factor = len(self.files_index) if settings.is_parallelism_enabled() else 1
            self.chunk_limit = psutil.virtual_memory().available * PERCENT_FREE_MEM_USE / mem_split_factor
            max_file_size = max([fe.decompressed_file_size_bytes for fi in self.files_index for fe in fi.entries])
            if max_file_size &gt; self.chunk_limit:
                raise MemoryError(f&#34;System requires {max_file_size} bytes of memory to process a file but only has &#34;
                                  f&#34;{self.chunk_limit} available.  Please free or add more RAM.&#34;)
            elif max_file_size * sum([len(fi.entries) for fi in self.files_index]) &gt; self.chunk_limit:
                raise MemoryError(&#34;TOO MUCH DATA DAWG&#34;)
            if debug:
                if mem_split_factor == 1:
                    print(f&#34;{len(self.files_index)} stations have {int(self.chunk_limit)} &#34;
                          f&#34;bytes for loading files in memory.&#34;)
                else:
                    print(f&#34;{mem_split_factor} stations each have &#34;
                          f&#34;{int(self.chunk_limit)} bytes for loading files in memory.&#34;)
        else:
            self.chunk_limit = 0

        if debug:
            self.errors.print()

        if pool is None:
            _pool.close()

    def _flatten_files_index(self):
        &#34;&#34;&#34;
        :return: flattened version of files_index
        &#34;&#34;&#34;
        result = io.Index()
        for i in self.files_index:
            result.append(i.entries)
        return result

    def _get_all_files(
        self, pool: Optional[multiprocessing.pool.Pool] = None
    ) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        get all files in the base dir of the ApiReader

        :return: index with all the files that match the filter
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = (
            multiprocessing.Pool() if pool is None else pool
        )
        index: List[io.Index] = []
        # this guarantees that all ids we search for are valid
        all_index = self._apply_filter(pool=_pool)
        for station_id in all_index.summarize().station_ids():
            id_index = all_index.get_index_for_station_id(station_id)
            checked_index = self._check_station_stats(id_index, pool=_pool)
            index.extend(checked_index)

        if pool is None:
            _pool.close()

        return index

    def _apply_filter(
        self,
        reader_filter: Optional[io.ReadFilter] = None,
        pool: Optional[multiprocessing.pool.Pool] = None,
    ) -&gt; io.Index:
        &#34;&#34;&#34;
        apply the filter of the reader, or another filter if specified

        :param reader_filter: optional filter; if None, use the reader&#39;s filter, default None
        :return: index of the filtered files
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = (
            multiprocessing.Pool() if pool is None else pool
        )
        if not reader_filter:
            reader_filter = self.filter
        if self.structured_dir:
            index = io.index_structured(self.base_dir, reader_filter, pool=_pool)
        else:
            index = io.index_unstructured(self.base_dir, reader_filter, pool=_pool)
        if pool is None:
            _pool.close()
        return index

    def _redo_index(self, station_ids: set, new_start: datetime, new_end: datetime):
        &#34;&#34;&#34;
        Redo the index for files using new start and end dates.  removes any buffer time at the start and end of the
        new query.  Returns the updated index or None

        :param station_ids: set of ids to get
        :param new_start: new start time to get data from
        :param new_end: new end time to get data from
        :return: Updated index or None
        &#34;&#34;&#34;
        diff_s = diff_e = timedelta(seconds=0)
        new_index = self._apply_filter(io.ReadFilter()
                                       .with_start_dt(new_start)
                                       .with_end_dt(new_end)
                                       .with_extensions(self.filter.extensions)
                                       .with_api_versions(self.filter.api_versions)
                                       .with_station_ids(station_ids)
                                       .with_start_dt_buf(diff_s)
                                       .with_end_dt_buf(diff_e))
        if len(new_index.entries) &gt; 0:
            return new_index
        return None

    def _check_station_stats(
            self,
            station_index: io.Index,
            pool: Optional[multiprocessing.pool.Pool] = None,
    ) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        check the index&#39;s results; if it has enough information, return it, otherwise search for more data.
        The index should only request one station id
        If the station was restarted during the request period, a new group of indexes will be created
        to represent the change in station metadata.

        :param station_index: index representing the requested information
        :return: List of Indexes that includes as much information as possible that fits the request
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = multiprocessing.Pool() if pool is None else pool
        # if we found nothing, return the index
        if len(station_index.entries) &lt; 1:
            return [station_index]

        stats = fs.extract_stats(station_index, pool=_pool)
        # Close pool if created here
        if pool is None:
            _pool.close()

        timing_offsets: Optional[offset_model.TimingOffsets] = offset_model.compute_offsets(stats)

        # punt if duration or other important values are invalid or if the latency array was empty
        if timing_offsets is None:
            return [station_index]

        # if our filtered files do not encompass the request even when the packet times are updated
        # try getting 1.5 times the difference of the expected start/end and the start/end of the data
        insufficient_str = &#34;&#34;
        if (self.filter.start_dt and timing_offsets.adjusted_start &gt; self.filter.start_dt) or \
                (self.filter.end_dt and timing_offsets.adjusted_start &gt;= self.filter.end_dt):
            insufficient_str += f&#34; {self.filter.start_dt} (start)&#34;
            new_end = self.filter.start_dt - self.filter.start_dt_buf
            new_start = new_end - 1.5 * (timing_offsets.adjusted_start - self.filter.start_dt)
            new_index = self._redo_index(set(station_index.summarize().station_ids()), new_start, new_end)
            if new_index:
                station_index.append(new_index.entries)
                stats.extend(fs.extract_stats(new_index))

        if (self.filter.end_dt and timing_offsets.adjusted_end &lt; self.filter.end_dt) or \
                (self.filter.start_dt and timing_offsets.adjusted_end &lt;= self.filter.start_dt):
            insufficient_str += f&#34; {self.filter.end_dt} (end)&#34;
            new_start = self.filter.end_dt + self.filter.end_dt_buf
            new_end = new_start + 1.5 * (self.filter.end_dt - timing_offsets.adjusted_end)
            new_index = self._redo_index(set(station_index.summarize().station_ids()), new_start, new_end)
            if new_index:
                station_index.append(new_index.entries)
                stats.extend(fs.extract_stats(new_index))

        if len(insufficient_str) &gt; 0:
            self.errors.append(f&#34;Data for {station_index.summarize().station_ids()} exists, &#34;
                               f&#34;but not at:{insufficient_str}&#34;)

        results = {}
        keys = []

        for v, e in enumerate(stats):
            key = e.app_start_dt
            if key not in keys:
                keys.append(key)
                results[key] = io.Index()

            results[key].append(entries=[station_index.entries[v]])

        return list(results.values())

    def _split_workload(self, findex: io.Index) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        takes an index and splits it into chunks based on a size limit
        while running_total + next_file_size &lt; limit, adds files to a chunk (Index)
        if limit is exceeded, adds the chunk and puts the next file into a new chunk

        :param findex: index of files to split
        :return: list of Index to process
        &#34;&#34;&#34;
        packet_list = []
        chunk_queue = 0
        chunk_list = []
        for f in findex.entries:
            chunk_queue += f.decompressed_file_size_bytes
            if chunk_queue &gt; self.chunk_limit:
                packet_list.append(io.Index(chunk_list))
                chunk_queue = 0
                chunk_list = []
            chunk_list.append(f)
        packet_list.append(io.Index(chunk_list))
        return packet_list

    @staticmethod
    def read_files_in_index(indexf: io.Index) -&gt; List[api_m.RedvoxPacketM]:
        &#34;&#34;&#34;
        read all the files in the index

        :return: list of RedvoxPacketM, converted from API 900 if necessary
        &#34;&#34;&#34;
        result: List[api_m.RedvoxPacketM] = []

        # Iterate over the API 900 packets in a memory efficient way
        # and convert to API 1000
        # noinspection PyTypeChecker
        for packet_900 in indexf.stream_raw(
                io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_900})
        ):
            # noinspection Mypy
            result.append(
                ac.convert_api_900_to_1000_raw(packet_900)
            )

        # Grab the API 1000 packets
        # noinspection PyTypeChecker
        for packet in indexf.stream_raw(
                io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_1000})
        ):
            # noinspection Mypy
            result.append(packet)

        return result

    # noinspection PyTypeChecker
    def read_files_by_id(self, station_id: str) -&gt; Optional[List[api_m.RedvoxPacketM]]:
        &#34;&#34;&#34;
        :param station_id: the id to filter on
        :return: the list of packets with the requested id, or None if the id can&#39;t be found
        &#34;&#34;&#34;

        result: List[api_m.RedvoxPacketM] = []

        # Iterate over the API 900 packets in a memory efficient way
        # and convert to API 1000
        for packet_900 in self._flatten_files_index().stream_raw(
            io.ReadFilter.empty()
            .with_api_versions({io.ApiVersion.API_900})
            .with_station_ids({station_id})
        ):
            # noinspection Mypy
            result.append(ac.convert_api_900_to_1000_raw(packet_900))

        # Grab the API 1000 packets
        for packet in self._flatten_files_index().stream_raw(
            io.ReadFilter.empty()
            .with_api_versions({io.ApiVersion.API_1000})
            .with_station_ids({station_id})
        ):
            # noinspection Mypy
            result.append(packet)

        if len(result) == 0:
            return None

        return result

    def _station_by_index(self, findex: io.Index) -&gt; Station:
        &#34;&#34;&#34;
        :param findex: index with files to build a station with
        :return: Station built from files in findex
        &#34;&#34;&#34;
        return Station.create_from_packets(self.read_files_in_index(findex))

    def get_stations(self, pool: Optional[multiprocessing.pool.Pool] = None) -&gt; List[Station]:
        &#34;&#34;&#34;
        :param pool: optional multiprocessing pool
        :return: List of all stations in the ApiReader
        &#34;&#34;&#34;
        return list(maybe_parallel_map(pool,
                                       self._station_by_index,
                                       self.files_index,
                                       chunk_size=1
                                       )
                    )

    def get_station_by_id(self, get_id: str) -&gt; Optional[List[Station]]:
        &#34;&#34;&#34;
        :param get_id: the id to filter on
        :return: list of all stations with the requested id or None if id can&#39;t be found
        &#34;&#34;&#34;
        result = [s for s in self.get_stations() if s.id() == get_id]
        if len(result) &lt; 1:
            return None
        return result</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="redvox.common.api_reader.ApiReader"><code class="flex name class">
<span>class <span class="ident">ApiReader</span></span>
<span>(</span><span>base_dir: str, structured_dir: bool = False, read_filter: <a title="redvox.common.io.ReadFilter" href="io.html#redvox.common.io.ReadFilter">ReadFilter</a> = None, debug: bool = False, pool: Optional[multiprocessing.pool.Pool] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Reads data from api 900 or api 1000 format, converting all data read into RedvoxPacketM for
ease of comparison and use.</p>
<h2 id="properties">Properties</h2>
<p>filter: io.ReadFilter with the station ids, start and end time, start and end time padding, and
types of files to read</p>
<p>base_dir: str of the directory containing all the files to read</p>
<p>structured_dir: bool, if True, the base_dir contains a specific directory structure used by the
respective api formats.
If False, base_dir only has the data files.
Default False.</p>
<p>files_index: io.Index of the files that match the filter that are in base_dir</p>
<p>index_summary: io.IndexSummary of the filtered data</p>
<p>debug: bool, if True, output additional information during function execution.
Default False.</p>
<p>Initialize the ApiReader object</p>
<p>:param base_dir: directory containing the files to read
:param structured_dir: if True, base_dir contains a specific directory structure used by the respective
api formats.
If False, base_dir only has the data files.
Default False.
:param read_filter: ReadFilter for the data files, if None, get everything.
Default None
:param debug: if True, output program warnings/errors during function execution.
Default False.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ApiReader:
    &#34;&#34;&#34;
    Reads data from api 900 or api 1000 format, converting all data read into RedvoxPacketM for
    ease of comparison and use.

    Properties:
        filter: io.ReadFilter with the station ids, start and end time, start and end time padding, and
        types of files to read

        base_dir: str of the directory containing all the files to read

        structured_dir: bool, if True, the base_dir contains a specific directory structure used by the
        respective api formats.  If False, base_dir only has the data files.  Default False.

        files_index: io.Index of the files that match the filter that are in base_dir

        index_summary: io.IndexSummary of the filtered data

        debug: bool, if True, output additional information during function execution.  Default False.
    &#34;&#34;&#34;

    def __init__(
        self,
        base_dir: str,
        structured_dir: bool = False,
        read_filter: io.ReadFilter = None,
        debug: bool = False,
        pool: Optional[multiprocessing.pool.Pool] = None,
    ):
        &#34;&#34;&#34;
        Initialize the ApiReader object

        :param base_dir: directory containing the files to read
        :param structured_dir: if True, base_dir contains a specific directory structure used by the respective
                                api formats.  If False, base_dir only has the data files.  Default False.
        :param read_filter: ReadFilter for the data files, if None, get everything.  Default None
        :param debug: if True, output program warnings/errors during function execution.  Default False.
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = (
            multiprocessing.Pool() if pool is None else pool
        )

        if read_filter:
            self.filter = read_filter
            if self.filter.station_ids:
                self.filter.station_ids = set(self.filter.station_ids)
        else:
            self.filter = io.ReadFilter()
        self.base_dir = base_dir
        self.structured_dir = structured_dir
        self.debug = debug
        self.errors = RedVoxExceptions(&#34;APIReader&#34;)
        self.files_index = self._get_all_files(_pool)
        self.index_summary = io.IndexSummary.from_index(self._flatten_files_index())
        if len(self.files_index) &gt; 0:
            mem_split_factor = len(self.files_index) if settings.is_parallelism_enabled() else 1
            self.chunk_limit = psutil.virtual_memory().available * PERCENT_FREE_MEM_USE / mem_split_factor
            max_file_size = max([fe.decompressed_file_size_bytes for fi in self.files_index for fe in fi.entries])
            if max_file_size &gt; self.chunk_limit:
                raise MemoryError(f&#34;System requires {max_file_size} bytes of memory to process a file but only has &#34;
                                  f&#34;{self.chunk_limit} available.  Please free or add more RAM.&#34;)
            elif max_file_size * sum([len(fi.entries) for fi in self.files_index]) &gt; self.chunk_limit:
                raise MemoryError(&#34;TOO MUCH DATA DAWG&#34;)
            if debug:
                if mem_split_factor == 1:
                    print(f&#34;{len(self.files_index)} stations have {int(self.chunk_limit)} &#34;
                          f&#34;bytes for loading files in memory.&#34;)
                else:
                    print(f&#34;{mem_split_factor} stations each have &#34;
                          f&#34;{int(self.chunk_limit)} bytes for loading files in memory.&#34;)
        else:
            self.chunk_limit = 0

        if debug:
            self.errors.print()

        if pool is None:
            _pool.close()

    def _flatten_files_index(self):
        &#34;&#34;&#34;
        :return: flattened version of files_index
        &#34;&#34;&#34;
        result = io.Index()
        for i in self.files_index:
            result.append(i.entries)
        return result

    def _get_all_files(
        self, pool: Optional[multiprocessing.pool.Pool] = None
    ) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        get all files in the base dir of the ApiReader

        :return: index with all the files that match the filter
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = (
            multiprocessing.Pool() if pool is None else pool
        )
        index: List[io.Index] = []
        # this guarantees that all ids we search for are valid
        all_index = self._apply_filter(pool=_pool)
        for station_id in all_index.summarize().station_ids():
            id_index = all_index.get_index_for_station_id(station_id)
            checked_index = self._check_station_stats(id_index, pool=_pool)
            index.extend(checked_index)

        if pool is None:
            _pool.close()

        return index

    def _apply_filter(
        self,
        reader_filter: Optional[io.ReadFilter] = None,
        pool: Optional[multiprocessing.pool.Pool] = None,
    ) -&gt; io.Index:
        &#34;&#34;&#34;
        apply the filter of the reader, or another filter if specified

        :param reader_filter: optional filter; if None, use the reader&#39;s filter, default None
        :return: index of the filtered files
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = (
            multiprocessing.Pool() if pool is None else pool
        )
        if not reader_filter:
            reader_filter = self.filter
        if self.structured_dir:
            index = io.index_structured(self.base_dir, reader_filter, pool=_pool)
        else:
            index = io.index_unstructured(self.base_dir, reader_filter, pool=_pool)
        if pool is None:
            _pool.close()
        return index

    def _redo_index(self, station_ids: set, new_start: datetime, new_end: datetime):
        &#34;&#34;&#34;
        Redo the index for files using new start and end dates.  removes any buffer time at the start and end of the
        new query.  Returns the updated index or None

        :param station_ids: set of ids to get
        :param new_start: new start time to get data from
        :param new_end: new end time to get data from
        :return: Updated index or None
        &#34;&#34;&#34;
        diff_s = diff_e = timedelta(seconds=0)
        new_index = self._apply_filter(io.ReadFilter()
                                       .with_start_dt(new_start)
                                       .with_end_dt(new_end)
                                       .with_extensions(self.filter.extensions)
                                       .with_api_versions(self.filter.api_versions)
                                       .with_station_ids(station_ids)
                                       .with_start_dt_buf(diff_s)
                                       .with_end_dt_buf(diff_e))
        if len(new_index.entries) &gt; 0:
            return new_index
        return None

    def _check_station_stats(
            self,
            station_index: io.Index,
            pool: Optional[multiprocessing.pool.Pool] = None,
    ) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        check the index&#39;s results; if it has enough information, return it, otherwise search for more data.
        The index should only request one station id
        If the station was restarted during the request period, a new group of indexes will be created
        to represent the change in station metadata.

        :param station_index: index representing the requested information
        :return: List of Indexes that includes as much information as possible that fits the request
        &#34;&#34;&#34;
        _pool: multiprocessing.pool.Pool = multiprocessing.Pool() if pool is None else pool
        # if we found nothing, return the index
        if len(station_index.entries) &lt; 1:
            return [station_index]

        stats = fs.extract_stats(station_index, pool=_pool)
        # Close pool if created here
        if pool is None:
            _pool.close()

        timing_offsets: Optional[offset_model.TimingOffsets] = offset_model.compute_offsets(stats)

        # punt if duration or other important values are invalid or if the latency array was empty
        if timing_offsets is None:
            return [station_index]

        # if our filtered files do not encompass the request even when the packet times are updated
        # try getting 1.5 times the difference of the expected start/end and the start/end of the data
        insufficient_str = &#34;&#34;
        if (self.filter.start_dt and timing_offsets.adjusted_start &gt; self.filter.start_dt) or \
                (self.filter.end_dt and timing_offsets.adjusted_start &gt;= self.filter.end_dt):
            insufficient_str += f&#34; {self.filter.start_dt} (start)&#34;
            new_end = self.filter.start_dt - self.filter.start_dt_buf
            new_start = new_end - 1.5 * (timing_offsets.adjusted_start - self.filter.start_dt)
            new_index = self._redo_index(set(station_index.summarize().station_ids()), new_start, new_end)
            if new_index:
                station_index.append(new_index.entries)
                stats.extend(fs.extract_stats(new_index))

        if (self.filter.end_dt and timing_offsets.adjusted_end &lt; self.filter.end_dt) or \
                (self.filter.start_dt and timing_offsets.adjusted_end &lt;= self.filter.start_dt):
            insufficient_str += f&#34; {self.filter.end_dt} (end)&#34;
            new_start = self.filter.end_dt + self.filter.end_dt_buf
            new_end = new_start + 1.5 * (self.filter.end_dt - timing_offsets.adjusted_end)
            new_index = self._redo_index(set(station_index.summarize().station_ids()), new_start, new_end)
            if new_index:
                station_index.append(new_index.entries)
                stats.extend(fs.extract_stats(new_index))

        if len(insufficient_str) &gt; 0:
            self.errors.append(f&#34;Data for {station_index.summarize().station_ids()} exists, &#34;
                               f&#34;but not at:{insufficient_str}&#34;)

        results = {}
        keys = []

        for v, e in enumerate(stats):
            key = e.app_start_dt
            if key not in keys:
                keys.append(key)
                results[key] = io.Index()

            results[key].append(entries=[station_index.entries[v]])

        return list(results.values())

    def _split_workload(self, findex: io.Index) -&gt; List[io.Index]:
        &#34;&#34;&#34;
        takes an index and splits it into chunks based on a size limit
        while running_total + next_file_size &lt; limit, adds files to a chunk (Index)
        if limit is exceeded, adds the chunk and puts the next file into a new chunk

        :param findex: index of files to split
        :return: list of Index to process
        &#34;&#34;&#34;
        packet_list = []
        chunk_queue = 0
        chunk_list = []
        for f in findex.entries:
            chunk_queue += f.decompressed_file_size_bytes
            if chunk_queue &gt; self.chunk_limit:
                packet_list.append(io.Index(chunk_list))
                chunk_queue = 0
                chunk_list = []
            chunk_list.append(f)
        packet_list.append(io.Index(chunk_list))
        return packet_list

    @staticmethod
    def read_files_in_index(indexf: io.Index) -&gt; List[api_m.RedvoxPacketM]:
        &#34;&#34;&#34;
        read all the files in the index

        :return: list of RedvoxPacketM, converted from API 900 if necessary
        &#34;&#34;&#34;
        result: List[api_m.RedvoxPacketM] = []

        # Iterate over the API 900 packets in a memory efficient way
        # and convert to API 1000
        # noinspection PyTypeChecker
        for packet_900 in indexf.stream_raw(
                io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_900})
        ):
            # noinspection Mypy
            result.append(
                ac.convert_api_900_to_1000_raw(packet_900)
            )

        # Grab the API 1000 packets
        # noinspection PyTypeChecker
        for packet in indexf.stream_raw(
                io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_1000})
        ):
            # noinspection Mypy
            result.append(packet)

        return result

    # noinspection PyTypeChecker
    def read_files_by_id(self, station_id: str) -&gt; Optional[List[api_m.RedvoxPacketM]]:
        &#34;&#34;&#34;
        :param station_id: the id to filter on
        :return: the list of packets with the requested id, or None if the id can&#39;t be found
        &#34;&#34;&#34;

        result: List[api_m.RedvoxPacketM] = []

        # Iterate over the API 900 packets in a memory efficient way
        # and convert to API 1000
        for packet_900 in self._flatten_files_index().stream_raw(
            io.ReadFilter.empty()
            .with_api_versions({io.ApiVersion.API_900})
            .with_station_ids({station_id})
        ):
            # noinspection Mypy
            result.append(ac.convert_api_900_to_1000_raw(packet_900))

        # Grab the API 1000 packets
        for packet in self._flatten_files_index().stream_raw(
            io.ReadFilter.empty()
            .with_api_versions({io.ApiVersion.API_1000})
            .with_station_ids({station_id})
        ):
            # noinspection Mypy
            result.append(packet)

        if len(result) == 0:
            return None

        return result

    def _station_by_index(self, findex: io.Index) -&gt; Station:
        &#34;&#34;&#34;
        :param findex: index with files to build a station with
        :return: Station built from files in findex
        &#34;&#34;&#34;
        return Station.create_from_packets(self.read_files_in_index(findex))

    def get_stations(self, pool: Optional[multiprocessing.pool.Pool] = None) -&gt; List[Station]:
        &#34;&#34;&#34;
        :param pool: optional multiprocessing pool
        :return: List of all stations in the ApiReader
        &#34;&#34;&#34;
        return list(maybe_parallel_map(pool,
                                       self._station_by_index,
                                       self.files_index,
                                       chunk_size=1
                                       )
                    )

    def get_station_by_id(self, get_id: str) -&gt; Optional[List[Station]]:
        &#34;&#34;&#34;
        :param get_id: the id to filter on
        :return: list of all stations with the requested id or None if id can&#39;t be found
        &#34;&#34;&#34;
        result = [s for s in self.get_stations() if s.id() == get_id]
        if len(result) &lt; 1:
            return None
        return result</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="redvox.common.api_reader_dw.ApiReaderDw" href="api_reader_dw.html#redvox.common.api_reader_dw.ApiReaderDw">ApiReaderDw</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="redvox.common.api_reader.ApiReader.read_files_in_index"><code class="name flex">
<span>def <span class="ident">read_files_in_index</span></span>(<span>indexf: <a title="redvox.common.io.Index" href="io.html#redvox.common.io.Index">Index</a>) ‑> List[src.redvox_api_m.redvox_api_m_pb2.RedvoxPacketM]</span>
</code></dt>
<dd>
<div class="desc"><p>read all the files in the index</p>
<p>:return: list of RedvoxPacketM, converted from API 900 if necessary</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def read_files_in_index(indexf: io.Index) -&gt; List[api_m.RedvoxPacketM]:
    &#34;&#34;&#34;
    read all the files in the index

    :return: list of RedvoxPacketM, converted from API 900 if necessary
    &#34;&#34;&#34;
    result: List[api_m.RedvoxPacketM] = []

    # Iterate over the API 900 packets in a memory efficient way
    # and convert to API 1000
    # noinspection PyTypeChecker
    for packet_900 in indexf.stream_raw(
            io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_900})
    ):
        # noinspection Mypy
        result.append(
            ac.convert_api_900_to_1000_raw(packet_900)
        )

    # Grab the API 1000 packets
    # noinspection PyTypeChecker
    for packet in indexf.stream_raw(
            io.ReadFilter.empty().with_api_versions({io.ApiVersion.API_1000})
    ):
        # noinspection Mypy
        result.append(packet)

    return result</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="redvox.common.api_reader.ApiReader.get_station_by_id"><code class="name flex">
<span>def <span class="ident">get_station_by_id</span></span>(<span>self, get_id: str) ‑> Optional[List[<a title="redvox.common.station.Station" href="station.html#redvox.common.station.Station">Station</a>]]</span>
</code></dt>
<dd>
<div class="desc"><p>:param get_id: the id to filter on
:return: list of all stations with the requested id or None if id can't be found</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_station_by_id(self, get_id: str) -&gt; Optional[List[Station]]:
    &#34;&#34;&#34;
    :param get_id: the id to filter on
    :return: list of all stations with the requested id or None if id can&#39;t be found
    &#34;&#34;&#34;
    result = [s for s in self.get_stations() if s.id() == get_id]
    if len(result) &lt; 1:
        return None
    return result</code></pre>
</details>
</dd>
<dt id="redvox.common.api_reader.ApiReader.get_stations"><code class="name flex">
<span>def <span class="ident">get_stations</span></span>(<span>self, pool: Optional[multiprocessing.pool.Pool] = None) ‑> List[<a title="redvox.common.station.Station" href="station.html#redvox.common.station.Station">Station</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>:param pool: optional multiprocessing pool
:return: List of all stations in the ApiReader</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_stations(self, pool: Optional[multiprocessing.pool.Pool] = None) -&gt; List[Station]:
    &#34;&#34;&#34;
    :param pool: optional multiprocessing pool
    :return: List of all stations in the ApiReader
    &#34;&#34;&#34;
    return list(maybe_parallel_map(pool,
                                   self._station_by_index,
                                   self.files_index,
                                   chunk_size=1
                                   )
                )</code></pre>
</details>
</dd>
<dt id="redvox.common.api_reader.ApiReader.read_files_by_id"><code class="name flex">
<span>def <span class="ident">read_files_by_id</span></span>(<span>self, station_id: str) ‑> Optional[List[src.redvox_api_m.redvox_api_m_pb2.RedvoxPacketM]]</span>
</code></dt>
<dd>
<div class="desc"><p>:param station_id: the id to filter on
:return: the list of packets with the requested id, or None if the id can't be found</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_files_by_id(self, station_id: str) -&gt; Optional[List[api_m.RedvoxPacketM]]:
    &#34;&#34;&#34;
    :param station_id: the id to filter on
    :return: the list of packets with the requested id, or None if the id can&#39;t be found
    &#34;&#34;&#34;

    result: List[api_m.RedvoxPacketM] = []

    # Iterate over the API 900 packets in a memory efficient way
    # and convert to API 1000
    for packet_900 in self._flatten_files_index().stream_raw(
        io.ReadFilter.empty()
        .with_api_versions({io.ApiVersion.API_900})
        .with_station_ids({station_id})
    ):
        # noinspection Mypy
        result.append(ac.convert_api_900_to_1000_raw(packet_900))

    # Grab the API 1000 packets
    for packet in self._flatten_files_index().stream_raw(
        io.ReadFilter.empty()
        .with_api_versions({io.ApiVersion.API_1000})
        .with_station_ids({station_id})
    ):
        # noinspection Mypy
        result.append(packet)

    if len(result) == 0:
        return None

    return result</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="redvox.common" href="index.html">redvox.common</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="redvox.common.api_reader.ApiReader" href="#redvox.common.api_reader.ApiReader">ApiReader</a></code></h4>
<ul class="">
<li><code><a title="redvox.common.api_reader.ApiReader.get_station_by_id" href="#redvox.common.api_reader.ApiReader.get_station_by_id">get_station_by_id</a></code></li>
<li><code><a title="redvox.common.api_reader.ApiReader.get_stations" href="#redvox.common.api_reader.ApiReader.get_stations">get_stations</a></code></li>
<li><code><a title="redvox.common.api_reader.ApiReader.read_files_by_id" href="#redvox.common.api_reader.ApiReader.read_files_by_id">read_files_by_id</a></code></li>
<li><code><a title="redvox.common.api_reader.ApiReader.read_files_in_index" href="#redvox.common.api_reader.ApiReader.read_files_in_index">read_files_in_index</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>